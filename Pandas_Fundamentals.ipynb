{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Pandas Fundamentals\n",
    "\n",
    "This notebook provides an introduction to the `pandas` package. This notebook also borrows heavily from the book *Data Science Handbook*, which was written by Jake VanderPlas and is available at https://jakevdp.github.io/PythonDataScienceHandbook/ (accessed 12/17/2019) and from the pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# What is Pandas?\n",
    "<a id=\"pandas\"> </a>\n",
    "\n",
    "The `pandas` package is one of the most popular Python tools for data management and manipulation. `pandas` is built *on top* of `numpy`. Thus, much of the functionality and methods that are available in `numpy` are also available in `pandas`. \n",
    "\n",
    "From https://en.wikipedia.org/wiki/Pandas_(software) (accessed 1/7/2018):\n",
    "\n",
    ">Pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. The name is derived from the term \"panel data\", an econometrics term for multidimensional, structured data sets. Major features of the library are:\n",
    "\n",
    "> - DataFrame object for data manipulation with integrated indexing.\n",
    "> - Tools for reading and writing data between in-memory data structures and different file formats.\n",
    "> - Data alignment and integrated handling of missing data.\n",
    "> - Reshaping and pivoting of data sets.\n",
    "> - Label-based slicing, fancy indexing, and subsetting of large data sets.\n",
    "> - Data structure column insertion and deletion.\n",
    "> - Group by engine allowing split-apply-combine operations on data sets.\n",
    "> - Data set merging and joining.\n",
    "> - Hierarchical axis indexing to work with high-dimensional data in a lower-dimensional data structure.\n",
    "> - Time series-functionality: Date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging.\n",
    "> The library is highly optimized for performance, with critical code paths written in Cython or C.\n",
    "\n",
    "The following code block uses the *import - as* approach to import Pandas. The alias *pd* is a standard convention. We also import NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list | grep pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Pandas Objects\n",
    "<a id=\"Pandas_Objects\"> </a>\n",
    "\n",
    "Pandas objects can be thought of as enhanced NumPy arrays. The two main Pandas objects are:\n",
    "1. The Pandas `Series`, which is a one-dimensional array of indexed data. Accessing a single column of a DataFrame results in a `Series` object.\n",
    "2. The Pandas `DataFrame`, which is two-dimensional array of indexed data. A `DataFrame` is a container for multiple series objects. \n",
    "\n",
    "The fact that Pandas objects are indexed allows us to lookup and access values by the index. Also, in contrast to NumPy arrays which are for storing numeric data, Pandas objects can store pretty much any Python data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `Series` Object\n",
    "To demonstrate the Pandas `Series` object, suppose we have the following 24 data points for sales over a 24 month period stored in a list object named `sales`. Also, we have a list object named `month` that stores the month that the sales occured in using the date format YYYY-MM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = ['2015-01-31', '2015-02-28', '2015-03-31', '2015-04-30',\n",
    "         '2015-05-31', '2015-06-30', '2015-07-31', '2015-08-31',\n",
    "         '2015-09-30', '2015-10-31', '2015-11-30', '2015-12-31',\n",
    "         '2016-01-31', '2016-02-29', '2016-03-31', '2016-04-30',\n",
    "         '2016-05-31', '2016-06-30', '2016-07-31', '2016-08-31',\n",
    "         '2016-09-30', '2016-10-31', '2016-11-30', '2016-12-31']\n",
    "\n",
    "sales = [ 872,  873,  990, 1129, \n",
    "          969,  964, 1148,  614,\n",
    "         1068, 1138, 1057,  748,\n",
    "         1255, 1016, 1058, 1064, \n",
    "         1229, 1060, 1144, 1119, \n",
    "         1188,  817,  638, 1205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar capabilities are found in NumPy (remember that Pandas is based on NumPy), but in many cases you may find NumPy limitations to be cumbersome. For example, it would be much easier to use month names AS the index rather than be limited to the integer index provided by NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of object is month? sales?\n",
    "sales_data = [month, sales]\n",
    "np_sales = np.array(sales_data)\n",
    "print(np_sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy, if you want sales (row 1) for April, 2015 you have to know which column is April (column 3)\n",
    "print(np_sales)\n",
    "print(\"April 2015 sales: \",np_sales[1,3])\n",
    "\n",
    "# If you want to format it for currency, you can use the format method\n",
    "print(\"April 2015 sales: ${:0,.2f}\".format(float(np_sales[1,3])).replace('$-','-$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses the two lists to create a Pandas `Series` that is stored in a variable named `sales_series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_series = pd.Series(sales, index = month)\n",
    "\n",
    "print(f'The sales_series series is\\n{sales_series}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can access individual values, or a range of values, in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sales for April 2015\n",
    "print(\"April sales:\", sales_series['2015-04-30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sales for April 2015 through December 2015 \n",
    "sales_series['2015-04-30':'2015-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the Series object using Integers\n",
    "# Get the first five rows (Index 0 - 4, not including 5)\n",
    "sales_series[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NumPy-like syntax, we can do computations on `Series` objects. The following code blocks provides examples.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total sales for June 2015 through December 2015 \n",
    "sales_series['2015-06-30':'2015-12-31'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean sales for June 2015 through December 2015 \n",
    "sales_series['2015-06-30':'2015-12-31'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum sales for June 2015 through December 2015 \n",
    "sales_series['2015-06-30':'2015-12-31'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `DataFrame` Object\n",
    "\n",
    "We will now look at Pandas `DataFrame` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the date_range function to populate a dataframe with a certain number of periods\n",
    "dates = pd.date_range('20200101', periods=12,freq='M')\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the DataFrame with random numbers and assign them to columns (A, B, C, D)\n",
    "df_random = pd.DataFrame(np.random.randn(12, 4), index=dates, columns=list('ABCD'))\n",
    "df_random.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a basic description of your DataFrame\n",
    "df_random.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by values\n",
    "df_random.sort_values(by='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display only the values, use the to_numpy() function\n",
    "df_random.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code provides four lists that provide information on the sales of two products, A and B,\n",
    "#   in three stores, W1, E1, and E2, over 10 years.\n",
    "\n",
    "stores_list = ['W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', \n",
    "               'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', 'W1', \n",
    "               'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', \n",
    "               'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', 'E1', \n",
    "               'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', \n",
    "               'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2', 'E2']\n",
    "\n",
    "dates_list = ['12/31/2007', '12/31/2008', '12/31/2009', '12/31/2010', '12/31/2011', '12/31/2012', \n",
    "              '12/31/2013', '12/31/2014', '12/31/2015', '12/31/2016', '12/31/2007', '12/31/2008', \n",
    "              '12/31/2009', '12/31/2010', '12/31/2011', '12/31/2012', '12/31/2013', '12/31/2014', \n",
    "              '12/31/2015', '12/31/2016', '12/31/2007', '12/31/2008', '12/31/2009', '12/31/2010', \n",
    "              '12/31/2011', '12/31/2012', '12/31/2013', '12/31/2014', '12/31/2015', '12/31/2016', \n",
    "              '12/31/2007', '12/31/2008', '12/31/2009', '12/31/2010', '12/31/2011', '12/31/2012', \n",
    "              '12/31/2013', '12/31/2014', '12/31/2015', '12/31/2016', '12/31/2007', '12/31/2008', \n",
    "              '12/31/2009', '12/31/2010', '12/31/2011', '12/31/2012', '12/31/2013', '12/31/2014', \n",
    "              '12/31/2015', '12/31/2016', '12/31/2007', '12/31/2008', '12/31/2009', '12/31/2010', \n",
    "              '12/31/2011', '12/31/2012', '12/31/2013', '12/31/2014', '12/31/2015', '12/31/2016']\n",
    "\n",
    "product_list = ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', \n",
    "                'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', \n",
    "                'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', \n",
    "                'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', \n",
    "                'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', \n",
    "                'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B']\n",
    "\n",
    "sales_list = [1957, 1968, 1983, 2001, 1894, 1850, 2045, 1860, 1784, 1856, \n",
    "              1891, 1822, 1846, 1903, 1924, 1897, 1890, 1858, 1871, 1880, \n",
    "              1858, 1909, 1977, 1717, 1751, 1797, 1804, 1845, 1895, 1713, \n",
    "              1904, 1812, 1733, 1868, 1872, 1909, 2034, 1856, 1813, 1806, \n",
    "              1862, 1800, 1840, 1882, 1819, 1854, 1716, 1845, 1877, 1879, \n",
    "              1696, 1783, 1799, 1852, 1793, 1877, 1687, 1824, 1839, 1889]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block first uses the list to construct a Python dictionary, uses the dictionary to define a Pandas `DataFrame` object named `sales_data`, and uses the `head()` method to print the first 10 rows of the `DataFrame`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>The <i>head()</i> function:</b> The <i>head()</i> function is a Pandas object method that prints the first five rows of a Pandas object by default. Specifying an integer argument instructs Pandas to return the specified number of rows from the beginning of the object. Similar to the <i>head()</i> method, the <i>tail()</i> method returns records from the end of a Pandas object.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dictionary = {'Store': stores_list,\n",
    "                    'Product': product_list,\n",
    "                    'Sales': sales_list,\n",
    "                    'Year': dates_list}\n",
    "# Although a dictionary is useful, it's hard to visualize and less convenient than a DataFrame\n",
    "print(sales_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.DataFrame.from_dict(sales_dictionary)\n",
    "\n",
    "sales_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key thing to note is that the format of the `DataFrame` object is very similar to what you would see when working with Microsoft Excel worksheets. \n",
    "\n",
    "There are many other ways to create Pandas objects from scratch. However, for the sake of space, we will stop here. See https://jakevdp.github.io/PythonDataScienceHandbook/03.01-introducing-pandas-objects.html or use Google to find other examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-nb-collapsed": true
   },
   "source": [
    "# Importing Data\n",
    "\n",
    "One of the most useful applications of Pandas is that it provides users with an easy way to import data from other sources. The following table was copied from https://pandas.pydata.org/pandas-docs/stable/io.html. Note that the hyperlinks will take you to sites descibing the different file formats or methods.\n",
    "\n",
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"12%\" />\n",
    "<col width=\"40%\" />\n",
    "<col width=\"24%\" />\n",
    "<col width=\"24%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Format Type</th>\n",
    "<th class=\"head\">Data Description</th>\n",
    "<th class=\"head\">Reader</th>\n",
    "<th class=\"head\">Writer</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">CSV</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table\"><span class=\"std std-ref\">read_csv</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-store-in-csv\"><span class=\"std std-ref\">to_csv</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"http://www.json.org/\">JSON</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader\"><span class=\"std std-ref\">read_json</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-json-writer\"><span class=\"std std-ref\">to_json</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-read-html\"><span class=\"std std-ref\">read_html</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-html\"><span class=\"std std-ref\">to_html</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>text</td>\n",
    "<td>Local clipboard</td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-clipboard\"><span class=\"std std-ref\">read_clipboard</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-clipboard\"><span class=\"std std-ref\">to_clipboard</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Microsoft_Excel\">MS Excel</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-excel-reader\"><span class=\"std std-ref\">read_excel</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-excel-writer\"><span class=\"std std-ref\">to_excel</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://support.hdfgroup.org/HDF5/whatishdf5.html\">HDF5 Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5\"><span class=\"std std-ref\">read_hdf</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5\"><span class=\"std std-ref\">to_hdf</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://github.com/wesm/feather\">Feather Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-feather\"><span class=\"std std-ref\">read_feather</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-feather\"><span class=\"std std-ref\">to_feather</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://parquet.apache.org/\">Parquet Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-parquet\"><span class=\"std std-ref\">read_parquet</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-parquet\"><span class=\"std std-ref\">to_parquet</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"http://msgpack.org/index.html\">Msgpack</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-msgpack\"><span class=\"std std-ref\">read_msgpack</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-msgpack\"><span class=\"std std-ref\">to_msgpack</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stata\">Stata</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-stata-reader\"><span class=\"std std-ref\">read_stata</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-stata-writer\"><span class=\"std std-ref\">to_stata</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SAS_(software)\">SAS</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sas-reader\"><span class=\"std std-ref\">read_sas</span></a></td>\n",
    "<td>&#160;</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html\">Python Pickle Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-pickle\"><span class=\"std std-ref\">read_pickle</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-pickle\"><span class=\"std std-ref\">to_pickle</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>SQL</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sql\"><span class=\"std std-ref\">read_sql</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-sql\"><span class=\"std std-ref\">to_sql</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>SQL</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/BigQuery\">Google Big Query</a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-bigquery\"><span class=\"std std-ref\">read_gbq</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"https://pandas.pydata.org/pandas-docs/stable/io.html#io-bigquery\"><span class=\"std std-ref\">to_gbq</span></a></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how the `read_csv` method that is available in Pandas can be used to read a file from the Internet that uses semi-colon delimiters, store it in an object named `my_data`, and use the `head()` method to print the first fives rows of the `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00409/Daily_Demand_Forecasting_Orders.csv',\n",
    "           delimiter = ';')\n",
    "\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After importing the data, it may be helpful to view the column data types\n",
    "#   Use the dtypes attribute\n",
    "my_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can also read directly from a compressed file. Here is the code for you to directly read a compressed file to a dataframe.\n",
    "```Python\n",
    "df = pd.read_csv('Sales_Data.csv.gz', compression = 'gzip')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from the clipboard\n",
    "\n",
    "Another method of obtaining data is to read it in from the clipboard. The clipboard is a location in memory where recently copied data can be accessed. It is where data are stored after cutting or copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy some table data \n",
    "import pandas as pd\n",
    "df = pd.read_clipboard(header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing and Selecting Data\n",
    "\n",
    "Although we have already seen some ways to access data in Pandas objects, this section will look at such methods in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Selecting Data in Pandas `Series` Objects\n",
    "\n",
    "Recall the `sales_series` object that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the number of columns Jupyter Lab will display at a time use set_option\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that the Pandas `Series` object acts like both a one-dimensional NumPy array and a standard Python dictionary. For example, to get index values for a `Series` object, you can use the `keys()` method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_series.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_series.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as shown earlier, we can use NumPy-like slicing as is demonstrated in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sales for June 2015 through December 2015 \n",
    "sales_series['2015-06-30':'2015-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the boolean array capabilities of NumPy to get subsets of the `Series` object that satisfy a specified condition. As an example, the following code block identifies the months with sales greater than or equal to 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which months had sales greater than $1,000? Show only the months, not the values by using keys().\n",
    "big_sales = sales_series[(sales_series > 1000)].keys()\n",
    "print(\"Number of months > $1,000: \", len(big_sales))\n",
    "print(big_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Selecting Data in Pandas `DataFrame` Objects\n",
    "\n",
    "We will now look at accessing data stored in a Pandas `DataFrame` object. Just as a reminder, the following code block prints the first five rows of the `sales_data` object that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the following code block, we can select an entire column of the `DataFrame` object, resulting in a `Series`, by providing the column name in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['Sales'][:5] #limit to the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can return multiple columns by providing the names of the desired columns as a list. Since we are returning multiple columns, the resulting object is a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data[['Product','Sales']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NumPy, we can perform calculations using entire columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sales_data['Sales']/365).apply(lambda x: \"${:.2f}\".format(x))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a subset of the available records by providing a boolean expression in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data[sales_data['Product']=='B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the data does not change the underlying dataset\n",
    "sales_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being build on top of NumPy, we can create more complicated boolean expressions to select subsets of records. The following code block shows how we can define and use multi-condition boolean expression to select records where the store is *E1* and the sales is greater than 1,850."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_condition = (sales_data['Store'] == 'E1') & (sales_data['Sales'] > 1850)\n",
    "\n",
    "sales_data[my_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using iloc (integer location) to access data\n",
    "Use `iloc` to access data within a DataFrame by passing an integer or list of integers.\n",
    "\n",
    "For example, if you want the first two rows of data pass an inner list to your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use integer location (iloc) to display the first and third rows--row 0 and row 2)\n",
    "sales_data.iloc[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display row index 2 and 3 and only the first (index 0) and third (index 2) columns\n",
    "sales_data.iloc[[2,3],[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using loc to access and change data\n",
    "Using the `.loc` *indexer* method for Pandas, we can use this boolean expression to return a subset of the available columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_condition = (sales_data['Store'] == 'E1') & (sales_data['Sales'] > 1850)\n",
    "\n",
    "## The following loc expression uses the boolean expression to filter the rows,\n",
    "## and then uses the list of column names to define the subset of columns to return\n",
    "sales_data.loc[my_condition,['Product','Year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see https://jakevdp.github.io/PythonDataScienceHandbook/03.02-data-indexing-and-selection.html for more discussion on indexing and selecting with Pandas objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Handling Missing Data\n",
    "\n",
    "In this section, we will look at a few capabilities built into Pandas for the purpose of handling missing data. For this demonstration, we will use a subset of the data contained in the `sales_data` object. The following code block creates and prints the data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset = sales_data.copy()\n",
    "my_condition = (my_subset['Product']=='A') & (my_subset['Store']=='W1')\n",
    "my_subset = my_subset[my_condition]\n",
    "my_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NaN\n",
    "For the sake of demonstration, assume that we were missing sales for the years 2011 and 2014. The following code block uses the `None` value to delete these sales values from the `my_subset` object. Note that in the resulting DatafFrame, the sales values are replaced with `NaN`, which stands for __Not a Number__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset.loc[my_subset['Year']=='12/31/2011', 'Sales'] = None\n",
    "\n",
    "my_subset.loc[my_subset['Year']=='12/31/2014', 'Sales'] = None\n",
    "\n",
    "my_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Fill Values\n",
    "In some situations, it may be reasonable to *forward fill* values whenever missing values are encountered. Essentially, the *forward filling* method use a preceding value in the DataFrame, and can be accomplished using the `fillna` Pandas method as shown in the following code block. **Note that this function can take an *axis* argument, specifying the direction of filling. In this case, we want to fill across the rows, or axis 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset.fillna(axis = 0, method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Fill Values\n",
    "In contrast, it is also possible to *back fill* values, using values that follow in the DataFrame. This approach can be accomplished as shown in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset.fillna(axis = 0, method = 'bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate Values\n",
    "Instead of forward or back filling missing values, we may also use the `interpolate` method to take the average of preceding and following values. This approach is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset.interpolate(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Values \n",
    "The final approach to handling missing values that we will cover is to drop the values. This can be done using the `dropna` method shown in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also drop missing values by specifying a condition that the records with missing values will fail. For example, the following code block uses the condition `my_subset['Sales']>0` to effectively drop the records with missing sales values (i.e., 2011 and 2014 sales). \n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_subset[my_subset['Sales']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Working with Dates and times in Pandas\n",
    "Pandas offers power and convenient featurs to handle dates and times. \n",
    "\n",
    "Properly read \n",
    "Filter by date time\n",
    "Group date time\n",
    "Plot using timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_store_file = Path.cwd() / 'files' / 'Microsoft_Store.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Microsoft Store rating data\n",
    "import pandas as pd\n",
    "try:\n",
    "    df_ms = pd.read_csv('files/Microsoft_Store.csv')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 5 rows\n",
    "df_ms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms[['Name','Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.loc[5:10, ['Date', 'Name', 'Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error: Name contains NA/NaN values\n",
    "python_books = df_ms.loc[df_ms['Name'].str.contains(\"Python\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore NA values by setting na = False\n",
    "python_books = df_ms.loc[df_ms['Name'].str.contains(\"Python\", case=False, na=False)]\n",
    "python_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Data\n",
    "Often when using a data set you will need to make values consistent. For example, in the ms_df DataFrame, one of the categoris is Deveoper Tools. In all other data sets within your organization a different term is used for that category. Instead of \"Developer Tools\", your organization has standardized on \"DevTools\". Consequently, you'll want to change every occurrence of \"Developer Tools\" to \"DevTools\". This is easily accomplished in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms[\"Category\"].replace(\"Developer Tools\",\"DevTools\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Python books (which are in the modified category)\n",
    "df_ms.loc[df_ms['Name'].str.contains(\"Python\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Data Types\n",
    "Call pandas.DataFrame.astype(dtype) with dtype as a dictionary containing mappings of column names to values to change the type of each column in pandas.DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date and day name of BookViewer entry\n",
    "# ERROR: This fails because .day_name is not a method for a string. \n",
    "try:\n",
    "    print(df_ms.loc[2, 'Date'])\n",
    "    print(df_ms.loc[2, 'Date'].day_name())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting columns to other data types\n",
    "If the date/time column is in a standard format the Pandas understand, it will convert it automatically. If not, you must provide the string format for Pandas to properly interpret and convert the data. See (https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#providing-a-format-argument) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert String to datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date string to a datetime object.\n",
    "\n",
    "# With formatting string (unnecessary for this example because date format is recognized by Pandas)\n",
    "df_ms['Date'] = pd.to_datetime(df_ms['Date'], format='%d-%m-%Y')\n",
    "\n",
    "# No formatting string provides, so Pandas will make its best attempt at conversion.\n",
    "#df_ms['Date'] = pd.to_datetime(df_ms['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date of Bookviewer entry\n",
    "# This succeeds because 'Date' was converted from a string to a datetime object \n",
    "try:\n",
    "    print(df_ms.loc[2, 'Date'])\n",
    "    print(df_ms.loc[2, 'Date'].day_name())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also perform date/time conversions when you load a dataset\n",
    "# Use the parse_dates parameter and pass the datetime columns of your dataset\n",
    "\n",
    "df_ms = pd.read_csv(ms_store_file, parse_dates=['Date'])\n",
    "print(df_ms.loc[2, 'Date'].day_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string or object to float\n",
    "\n",
    "Syntax: \n",
    "```Python\n",
    "pandas.to_numeric(arg, errors=’raise’, downcast=None)\n",
    "```\n",
    "Parameters:\n",
    "\n",
    "arg : list, tuple, 1-d array, or Series\n",
    "\n",
    "errors : {‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’\n",
    "* If ‘raise’, then invalid parsing will raise an exception\n",
    "* If ‘coerce’, then invalid parsing will be set as NaN\n",
    "* If ‘ignore’, then invalid parsing will return the input\n",
    "\n",
    "\n",
    "downcast : [default None] If not None, and if the data has been successfully cast to a numerical dtype downcast that resulting data to the smallest numerical dtype possible according to the following rules:\n",
    "* ‘integer’ or ‘signed’: smallest signed int dtype (min.: np.int8)\n",
    "* ‘unsigned’: smallest unsigned int dtype (min.: np.uint8)\n",
    "* ‘float’: smallest float dtype (min.: np.float32)\n",
    "\n",
    "Returns: numeric if parsing succeeded. Note that return type depends on input. Series if Series, otherwise ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types before conversion\n",
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows\n",
    "df_ms.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the last few rows\n",
    "df_ms.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check items that aren't free\n",
    "df_ms[df_ms['Price']!='Free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does a particular column have any NaN values?\n",
    "df_ms['Price'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the DataFrame have any NaN values?\n",
    "df_ms.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many NaN values are in the DataFrame (by column)?\n",
    "df_ms.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many NaN values are in the DataFrame (entire DataFrame)?\n",
    "df_ms.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the NaN rows\n",
    "df_ms[df_ms['Price'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete (drop) the row using its index number\n",
    "df_ms.drop(5321, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique prices \n",
    "#   (not feasible to display all unique prices for a large data set)\n",
    "df_ms['Price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully consider (and test) the error parameter. \n",
    "#   What if we used 'ignore'? Valid? Ideal output? 'coerce'?\n",
    "df_ms['Price'] = pd.to_numeric(df_ms['Price'], downcast='float', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll assume that null prices are Free apps and replace NaN with Zero\n",
    "df_ms['Price'] = df_ms['Price'].replace([np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want the column to be a float, let's replace all the 'Free' prices with Zero\n",
    "df_ms['Price'] = df_ms['Price'].replace('[F|f]ree', 0, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion failed earlier because of the symbol. Let's remove it.\n",
    "df_ms['Price'] = df_ms['Price'].str.replace('₹ ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique prices\n",
    "df_ms['Price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully consider (and test) the error parameter. \n",
    "#   What if we used 'ignore'? Valid? Ideal output? 'coerce'?\n",
    "df_ms['Price'] = pd.to_numeric(df_ms['Price'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm Price is a float. What type of float is it? Why?\n",
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `dt` class\n",
    "The `dt` class on the Series object provides useful date and time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DayOfWeek column and store the named day\n",
    "df_ms['DayOfWeek'] = df_ms['Date'].dt.day_name()\n",
    "df_ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get earliest date\n",
    "print('Earlist date: ;', df_ms['Date'].min())\n",
    "\n",
    "# Get latest date\n",
    "print('Latest date: ', df_ms['Date'].max())\n",
    "\n",
    "# Get number of days between earliest and latest date\n",
    "print(df_ms['Date'].max() - df_ms['Date'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Microsoft Store records by date\n",
    "# Show only records after 2018\n",
    "date_filter = (df_ms['Date'] >= '2018')\n",
    "df_ms.loc[date_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Microsoft Store records by date\n",
    "# Show only records in 2018\n",
    "date_filter = (df_ms['Date'] >= '2018') & (df_ms['Date'] < '2019')\n",
    "df_ms.loc[date_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Microsoft Store records by date\n",
    "# Show only records in December of 2018\n",
    "date_filter = (df_ms['Date'] >= pd.to_datetime('2018-12-01')) & (df_ms['Date'] <= pd.to_datetime('2018-12-31'))\n",
    "df_dec_2018 = df_ms.loc[date_filter]\n",
    "df_dec_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average item rating for December\n",
    "print(\"Average item rating in Dec 2018: \", df_dec_2018['Rating'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Quarter column and a week column using dt.to_period\n",
    "df_ms['Quarter'] = df_ms['Date'].dt.to_period('Q')\n",
    "df_ms['WeekOfYear'] = df_ms['Date'].dt.to_period('W')\n",
    "df_ms\n",
    "\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects for more objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the columsn contained the the Microsoft Store Dataset\n",
    "df_ms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_num_ratings = (df_ms['No of people Rated'] > 999)\n",
    "\n",
    "# To include multiple columns in the result, use a list.\n",
    "df_ms.loc[high_num_ratings,['Name', 'No of people Rated','Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the 'Books' category using the tilde character\n",
    "high_num_ratings = (df_ms['No of people Rated'] > 997) & ~(df_ms['Category'] =='Books')\n",
    "\n",
    "# To include multiple columns in the result, use a list.\n",
    "df_ms.loc[high_num_ratings,['Name', 'No of people Rated','Category','Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include certain categories. Use isin() instead of multiple OR statements\n",
    "include_categories = ['Books', 'Lifestyle', 'Social', 'Music']\n",
    "high_num_ratings = (df_ms['No of people Rated'] > 997) & (df_ms['Category'].isin(include_categories))\n",
    "                                                          \n",
    "# To include multiple columns in the result, use a list.\n",
    "df_ms.loc[high_num_ratings,['Name', 'No of people Rated','Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Top 20 items by # of people rated\n",
    "df_ms.nlargest(20, ['No of people Rated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for partial string within a column\n",
    "#   To avoid None (NaN) errors, set na=False\n",
    "string_filter = df_ms['Name'].str.contains('Psyc'.title(),na=False)\n",
    "df_ms.loc[string_filter, \"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ms['Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's happening? The filter results in a True/False 'mask' applied to the data\n",
    "#   Since only 5 of more than 5,000 app names contain the string 'Pysc' all visible mask items are False\n",
    "print(string_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and Aggregating Data\n",
    "\n",
    "In this section, we will look at how to use the `groupby` Pandas method to aggregate data that resides in a Pandas DataFrame object. Essentiall, the `groupby` method allows us to define a subset of the columns in a DataFrame to aggregate on. We can then use defined and user-specifed functions to compute aggregate statistics for the aggregated data. The following table, modified from that found at https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html, describes some of the defined aggregations.\n",
    "\n",
    "<table>\n",
    "<thead><tr>\n",
    "<th>Aggregation</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><code>count()</code></td>\n",
    "<td>Total number of items</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>first()</code>, <code>last()</code></td>\n",
    "<td>First and last item</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>mean()</code>, <code>median()</code></td>\n",
    "<td>Mean and median</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>min()</code>, <code>max()</code></td>\n",
    "<td>Minimum and maximum</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>std()</code>, <code>var()</code></td>\n",
    "<td>Standard deviation and variance</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>mad()</code></td>\n",
    "<td>Mean absolute deviation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>prod()</code></td>\n",
    "<td>Product of all items</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>sum()</code></td>\n",
    "<td>Sum of all items</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>unique()</code></td>\n",
    "<td>Unique values</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>nunique()</code></td>\n",
    "<td>Number of unique values</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "The following code block demonstrates how to use the `groupby` method to compute the total sales for each of the stores included in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would you group by product and display the total sales for each product?\n",
    "sales_data.groupby(['Product'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block provides a closer look at how the `groupby` method works. Specifically, if we convert the grouped object to a list, we see that we get a list of tuples. The first value in the tuple specifies the grouping, and the second item specifies the data that belongs to the associated group.\n",
    "\n",
    "[Back to Table of Contents](#Table_of_Contents)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sales_data.groupby(['Store']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a single column is specified for grouping, any aggregations are returned as a `Series` object. Thus, we can access the values using the indexing and selection methods for `Series` objects. For example, the following code block shows how to get the total sales for store *E1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store'])['Sales'].sum()['E1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more than one column is specified for grouping, aggregated data is again returned as a `Series` object. However, this object will utilize a *multi-level* index as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'])['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the total sales for store *E1* during 2007, we first need to get the data for store *E1*, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'])['Sales'].sum()['E1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can index into the resulting `Series` to get the 2007 sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'])['Sales'].sum()['E1']['12/31/2007']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, when multiple columns are grouped on, the resulting aggregations can be accessed in a manner that is similar to a multi-level dictionary. If we would like the object returned to resemble a DataFrame instead, we can set the optional `as_index` argument to `False`. This is demonstrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'], as_index = False)['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to grouping on several columns, we can also perform several aggregations using the `agg` method. The following code block shows an example where we group the `sales_data` object by *Store* and *Year*, and then determine the *mean*, *total*, and *maximum* sales for each grouping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'], as_index = False)['Sales'].agg({'mean','sum','max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the previous code block, even though we specified `as_index = False`, the returned object still utilizes a multi-level index. However, by *chaining* the expression with the `reset_index()` method, we can obtain a standard DataFrame object as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'], as_index = False)['Sales'].agg({'mean','sum','max'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also customize the column names as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'], as_index = False)['Sales'].agg({'Mean Sales':'mean',\n",
    "                                                                     'Total Sales':'sum',\n",
    "                                                                     'Maximum Sales':'max'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in addition to using the defined aggregations, a user can specify custom aggregations using a *lambda function*. A *lambda function* is essentially a function without a defined name. To apply a lambda function, we use the syntax `lambda my_var: my_func(my_var)`, where `my_var` represents a variable that will be passed as an argument and `myfunc()` represents a function that receives `my_var` as an argument. In the case of our groupby object, the values belonging to each group will be passed as the variable.\n",
    "\n",
    "The following code block shows how to use a lambda function to compute the $90^{th}$ percentile of the sales for each store, using NumPy's `percentile` method, along with the *mean*, *total*, and *maximum* sales for each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby(['Store','Year'], as_index = False)['Sales'].agg({'Mean Sales':'mean',\n",
    "                                                                     'Total Sales':'sum',\n",
    "                                                                     'Maximum Sales':'max',\n",
    "                                                                     '90% Sales': lambda x: np.percentile(x, 90)}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to perform different aggregations for different variables. However, it becomes quite complex to work with the returned objects. FOr more information, see https://pandas.pydata.org/pandas-docs/stable/groupby.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Tables\n",
    "\n",
    "The previous section showed how the `groupby` functionality available in Pandas allows you to aggregate data that resides in a `DataFrame` object in a very flexible manner. However, many users are very familiar with the aggregation capabilities made available by Microsoft Excel's pivot table functionality. Noting this, the Pandas package comes with a builtin `pivot_table` method that attempts to replicate this functionality in a simple fashion.\n",
    "\n",
    "The following code block shows how to use this function to compute the `mean` sales for each store, by year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(sales_data, values = 'Sales', columns = 'Store', index = 'Year', aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how to use this function to compute the total sales for each store, by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(sales_data, values = 'Sales', columns = 'Store', index = 'Year', aggfunc = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_mapping = {\"Sales\": \"${:,.2f}\", \"E2\": \"{:,.0f}\", \"W1\": \"{:.2f}%\"}\n",
    "\n",
    "sales_data.style.format(format_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['Store'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets\n",
    "\n",
    "The final thing that we consider in this notebook is how to merge DataFrame objects. In particular, let's assume that we have another DataFrame object that includes information on each of the stores. The following code block creates such an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_list = ['W1', 'E1', 'E2']\n",
    "\n",
    "city_list = ['City A', 'City B', 'City C',]\n",
    "\n",
    "state_list = ['State 1', 'State 2', 'State 2']\n",
    "\n",
    "my_dict = {'Store': stores_list,\n",
    "           'City': city_list,\n",
    "           'State': state_list}\n",
    "\n",
    "store_info = pd.DataFrame.from_dict(my_dict)\n",
    "\n",
    "store_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we want the total sales by state. To calculate this quantity, we first need to group the data in the `sales_data` object by store. This grouping is performed in the following code block, and the resulting object is stored in a new object called `grouped_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = sales_data.groupby('Store', as_index = False)['Sales'].sum()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Pandas `merge` method to merge the `grouped_data` object with the `store_info` object. Since they both have a column that is named *Store*, the method will merge on this common column. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.merge(store_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now *chain* the `groupby` method to the end of the merged data to obtain the total sales by store as is shown in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.merge(store_info).groupby('State', as_index = False)['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we could have performed the merging and aggregation in a single, *chained* statement as follows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.groupby('Store', as_index = False)['Sales'].sum()\\\n",
    ".merge(store_info).groupby('State', as_index = False)['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the case where we don't have nformation for all of the stores. The following code block creates a new version of the `store_info` object where we only have information for stores *E1* and *W1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_list = ['W1', 'E1', ]\n",
    "\n",
    "city_list = ['City A', 'City B',]\n",
    "\n",
    "state_list = ['State 1', 'State 2']\n",
    "\n",
    "my_dict = {'Store': stores_list,\n",
    "           'City': city_list,\n",
    "           'State': state_list}\n",
    "\n",
    "store_info2 = pd.DataFrame.from_dict(my_dict)\n",
    "\n",
    "store_info2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows the result of using the `merge` method to combine the `grouped_data` object with our new `store_info2` object. As you can see, with the default arguments, the returned object only includes records for stores included in both objects. Actually, the `method` is performing what is known as a *inner* where only records in both DataFrame objects are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.merge(store_info2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted the object returned to include records for all stores, even if one is missing in one of the DataFrames being joined, we can specify an *outer* join using the `how` argument. This is shown in the following code block. Since he `store_info2` object does not include any information for store *E2*, we will get a `NaN` value for the *City* and *State* columns of the associated row in the returned object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data.merge(store_info2, how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas package includes additional capabilities for joining DataFrame objects. For more info, see https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
